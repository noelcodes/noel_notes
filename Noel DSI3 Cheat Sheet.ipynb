{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler #statsmodels api module\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets.csv', keep_default_na=False)\n",
    "\n",
    "df = pd.read_csv('../datasets.tsv', sep='\\t', na_values={'is_news' : '?'}).fillna(0)\n",
    "\n",
    "df.to_csv(file_name)  # pd to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.get_dtype_counts()\n",
    "df.dtypes\n",
    "df.COL.unique()\n",
    "df.isnull().sum()\n",
    "type(df.CRIM.values)\n",
    "\n",
    "df.index\n",
    "df.shape\n",
    "df.DIS[0:30].values\n",
    "pd.value_counts(series)\n",
    "df['col'].round(decimals=1).value_counts()\n",
    "df.columns\n",
    "df.describe()\n",
    "df.corr()\n",
    "np.corrcoef(matrix, y, bias=True)\n",
    "np.cov(matrix, y, bias=True)\n",
    "df.col.sort_values(ascending=False)\n",
    "\n",
    "for item in df:   #Return number of unique elements in the object\n",
    "    print item, df[item].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = len(df) * 0.2                                    # Drop columns with 80% null values\n",
    "df.dropna(thresh = thresh, axis = 1, inplace = True)      # Drop columns with 80% null values\n",
    "df_numeric = df.select_dtypes(exclude=[object])           # Useful if df have mixture of cat and num, this pulls only int+float\n",
    "df_cat = df.select_dtypes(include=[object])               # Useful if df have mixture of cat and num, this pulls only cat\n",
    "df['col1'] = pd.to_numeric(df['col1'], errors='coerce', downcast='integer') # If df have mixture of num and string, convert all string to NAN\n",
    "df.col1.replace(np.nan, 'NO_Bas', regex=True)            # replace rows with Nan with SOMETHING\n",
    "df['col'].value_counts()                            # See values counts\n",
    "df1.join(df2)                                       # joining 2x df\n",
    "cols.difference(col1)                                # output difference of 2x columns\n",
    "df2 = df.columns.get_values()                       # column names to a list\n",
    "\n",
    "# works like dummy but for binary only, YES/NO\n",
    "df['col1'] = df['col1'].map(lambda x: 1 if x == 'yes' else 0 if x == 'no' else x)\n",
    "\n",
    "df.notnull().sum()\n",
    "\n",
    "df.DIS = df.DIS.map(lambda x : float('.'.join(x.split(','))))   # Remove DIS column ',' with '.'\n",
    "df.RAD = df.RAD.map(lambda x: np.nan if x == '?' else float(x))  # Turn fload Noel not sure\n",
    "df = df[[c for c in df.columns if not '_worst' in c and not '_se' in c]]  # Remove columns with specific words\n",
    "df.dropna(inplace=True)     \n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "boston.DIS = boston.DIS.map(lambda x: float(x.replace(',','.')))   # like join 2 list into 1\n",
    "df.rename(columns={'CRIM':'rate_of_crime', 'ZN':'residential_zone_pct'}, inplace=True)   # Rename column via Dictionary method\n",
    "df.apply(np.sqrt)\n",
    "df.groupby('col1')\n",
    "df.unstack()   # transpost from long df to WIde df\n",
    "df = df[~df.col1.isnull() & ~df.col2.isnull()] # '~' drop rows with value null \n",
    "\n",
    "np.concatenate([vec1_2d, vec2_2d], axis=0)  # \"Vertical Stack\"\n",
    "np.concatenate([vec1_2d, vec2_2d], axis=1) #\"Horizontal Stack\"\n",
    "np.hstack([arr,arra])\n",
    "np.vstack((arr, arrb))\n",
    "\n",
    "pd.concat([df1, df2], axis=0)\n",
    "pd.concat([df1, df2], axis=1)\n",
    "pd.merge(df_a, df_b, on='subject_id', how='left')  # left, right, outer, inner\n",
    "\n",
    "\n",
    "# standardized_variable = (variable - mean_of_variable) / std_dev_of_variable\n",
    "#  forces a mean of 0 and a standard deviation of 1.\n",
    "roc = boston.rate_of_crime.values\n",
    "roc_mean = np.mean(roc)\n",
    "roc_std = np.std(roc)\n",
    "\n",
    "# Useful def, to replace null with medians\n",
    "def impute_medians(df, cols=[]):\n",
    "    for col in cols:\n",
    "        mval = df[col].median()\n",
    "        df.loc[df[col].isnull(), col] = mval\n",
    "    return df\n",
    "\n",
    "food = food.groupby('chain').apply(impute_medians, cols=['wagest','nmgrs','nregs','emp'])\n",
    "\n",
    "# YOu may drop variable with small variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dummy trap: Convert df with categorical variables to integer 0/1. Example Male/Female\n",
    "df = pd.concat([df, pd.get_dummies(df['cat_col'])], axis=1)\n",
    "\n",
    "# Fix dummy trap 2, if on;binary\n",
    "df['malignant'] = df['malignant'].map(lambda x: 0 if x == \"B\" else 1)\n",
    "\n",
    "# Pull data X = 2D , y = 1D\n",
    "X = df.iloc[ : ,  1:2].values  # 1:2 turn it into 2D, size (10,1)\n",
    "y = df.iloc[ : ,  2].values    # 2, turn it into 1D, size (10,0)\n",
    "\n",
    "# Splitting the dataset into Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Splitting the dataset into Training set and Test set\n",
    "skf = StratifiedKFold(n_splits=5)   # StratifiedKFold split by indices, different from cv, cross_val_score has include StratifiedKFold inside itself.\n",
    "cv_indices = skf.split(Xs, y)\n",
    "cv_indices = [[train,test] for train,test in cv_indices] # there will be 5 sets to put into a function for fit\n",
    "\n",
    "# If MLR, must add X0 column with all values 1, before modelling, else X0 is assumned missing by default.\n",
    "X = np.append(arr=np.ones((50,1)).astype(int), values=X, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling (Standardisation and Normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to do scaling? \n",
    "# Do .describe(), see all the mean, if values almost the same, then no need scalling\n",
    "\n",
    "# Standardisation  Mean = 0, Std=-1.xx to +1.xx  (StandardScaler)\n",
    "from sklearn.preprocessing import StandardScaler  # vs robustscaler, rbust removes outliers, which works beter\n",
    "sc_X = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "# Normalisation (0 to +1) (MinMaxScaler)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "Xn = mms.fit_transform(X-train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Baseline before dive into modelling\n",
    "\n",
    "lr = LinearRegression()                # for regression problem\n",
    "logreg = LogisticRegression()         # for classification problem\n",
    "k_means = KMeans().fit(df_numeric)    # for clustering problem\n",
    "\n",
    "print cross_val_score(lr, Xs, y, cv=10).mean()\n",
    "print cross_val_score(logreg, Xs, y, cv=10).mean()\n",
    "\n",
    "predicted = k_means.labels_\n",
    "print silhouette_score(df_numeric, predicted, metric='euclidean') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Method 1:\n",
    "-SelectKbest: Univariate feature selection works by using statistical test. \n",
    "-SelectKBest for regression : f_regression, mutual_info_regression\n",
    "-SelectKbest for classification: f_classif, chi2, mutual_info_classif\n",
    "    \n",
    "Method 2:\n",
    "RFECV: Recursive feature elimination, u choose any model, REFCV reduces features,re-fits the model until it gives the best cv score. \n",
    "\n",
    "Method 3:\n",
    "Penalty method.\n",
    "LassoCV : For regression\n",
    "LogisticRegressionCV : For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noel's stupid mistakes. For code X.columns below to work, Xs = df.values | y = df.values |  X = df | Xs and X same no. of columns\n",
    "\n",
    "# For Regression Feature Selection\n",
    "######################################################################################\n",
    "# Method 1: SelectKBest\n",
    "\n",
    "# SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "f_reg= SelectKBest(f_regression, k=20)    # Noel, k is no. of top feature to select\n",
    "f_reg.fit(Xs, y)                        \n",
    "df_f_reg = pd.DataFrame([X.columns, list(f_reg.scores_)], \n",
    "                     index=['feature','f_reg']).T.sort_values('f_reg', ascending=False)\n",
    "f_reg_features = df_f_reg[df_f_reg.f_reg > 110].feature.values   # Noel choose 110, from scores, adjust accordingly\n",
    "\n",
    "print 'f_reg_features :', f_reg_features \n",
    "print 'No. of features selected : ' , f_reg_features.size\n",
    "df_f_reg.head(30)   # print df \n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# SelectKBest, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "mui_reg= SelectKBest(mutual_info_regression, k=20)    # Noel, k is no. of top feature to select\n",
    "mui_reg.fit(Xs, y)                        \n",
    "df_mui_reg = pd.DataFrame([X.columns, list(mui_reg.scores_)], \n",
    "                     index=['feature','mui_reg']).T.sort_values('mui_reg', ascending=False)\n",
    "mui_reg_features = df_mui_reg[df_mui_reg.mui_reg > 0.118].feature.values   # Noel choose 0.115, from scores, adjust accordingly\n",
    "\n",
    "print 'mui_reg_features :', mui_reg_features \n",
    "print 'No. of features selected : ' , mui_reg_features.size\n",
    "df_mui_reg.head(20)   # print df \n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "selector = RFECV(lr, step=1, cv=10)  # step=% of features to remove, cv=no. of cross_validation folds for evalution.\n",
    "selector = selector.fit(Xs, y)\n",
    "# print selector.support_   # Output True/False\n",
    "# print selector.ranking_   # Output  Ranking in Numbers\n",
    "rfecv_columns = np.array(X.columns)[selector.support_]\n",
    "print 'No. of rfecv_columns:' , rfecv_columns.size\n",
    "print 'rfecv_columns:' , rfecv_columns\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Method 3:\n",
    "# LassoCV for Regression\n",
    "print 'Before feature selection' , Xs.shape\n",
    "optimal_lasso = LassoCV(n_alphas=500, cv=10, verbose=1, n_jobs=-1).fit(Xs, y)\n",
    "lasso = Lasso(alpha=optimal_lasso.alpha_).fit(Xs, y)\n",
    "LassoCVscore = cross_val_score(lasso, Xs, y, cv=10)\n",
    "print 'Lasso Mean scores :' , LassoCVscore.mean()\n",
    "print 'No. of features selected : ' , np.count_nonzero(lasso.coef_)\n",
    "\n",
    "# Print out the selected features.\n",
    "lasso_coefs = pd.DataFrame({'variable':X.columns,\n",
    "                            'coef':lasso.coef_,\n",
    "                            'abs_coef':np.abs(lasso.coef_)  })\n",
    "lasso_coefs.sort_values('abs_coef', inplace=True, ascending=False)\n",
    "\n",
    "lasso_best_features = lasso_coefs[lasso_coefs.abs_coef != 0].variable.values\n",
    "print 'lasso_best_features : ',lasso_best_features\n",
    "lasso_coefs [lasso_coefs.abs_coef !=0]\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# For Classification Feature Selection\n",
    "\n",
    "#------------------\n",
    "\n",
    "# Method 3: LogisticRegressionCV + Lasso L1 , for Classification\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lrcv = LogisticRegressionCV(penalty='l1', Cs=100, cv=10, solver='liblinear')\n",
    "lrcv.fit(Xs, y)\n",
    "lrcv.C_    \n",
    "# What are the best coefficients according to a model using lasso?\n",
    "coeffs = pd.DataFrame(lrcv.coef_, columns=X.columns)\n",
    "coeffs_t = coeffs.transpose()\n",
    "coeffs_t.columns = ['lasso_coefs']\n",
    "coeffs_abs = coeffs_t.abs().sort_values('lasso_coefs', ascending=False)\n",
    "coeffs_abs    # pick the higher coeffs values, just means that it has more effect on y\n",
    "\n",
    "\n",
    "# Lastly after done ALL above, compare coeffs_abs vs rfecv_columns vs kbest vs all columns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=lrcv.C_[0], penalty='l1', solver='liblinear')\n",
    "def score(X): # Defining a function to test our best features head to head.\n",
    "    scores = cross_val_score(lr, X, y, cv=5)\n",
    "    return scores.mean(), scores.std()\n",
    "all_scores = [\n",
    "    score(X[kbest_columns]),\n",
    "    score(X[rfecv_columns]),\n",
    "    score(X[lasso_columns]),\n",
    "    score(X)] # A list of all of our lists of best features being executed in the score function.\n",
    "pd.DataFrame(all_scores, columns=['mean score', 'std score'], index = ['kbest', 'rfecv', 'lr', 'all']) # Putting results into a DataFrame.\n",
    "\n",
    "# Noel choose the mean value closer to 1 is the best method to slect features. \n",
    "mean score\tstd score\n",
    "kbest\t0.776691\t0.019541\n",
    "rfecv\t0.787832\t0.020027\n",
    "lr\t0.780055\t0.019567\n",
    "all\t0.781166\t0.020190\n",
    "\n",
    "\n",
    "##########################################\n",
    "LassoCV from Youtube\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "m = SelectFromModel((LassoCV))\n",
    "m.fit(X,y)\n",
    "m.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "**SLR AND MLR**:  Simple and Multiple Linear regression Assumption\n",
    "- *Linearity: Y must have an approximately linear relationship with each independent X_i.*\n",
    "- *Independence: Errors (residuals) e_i and e_j must be independent of one another for any i != j.*\n",
    "- *Normality: The errors (residuals) follow a Normal distribution.*\n",
    "- *Equality of Variances: The errors (residuals) should have a roughly consistent pattern, regardless of the value of the X_i. (There should be no discernable relationship between X_1 and the residuals.)*\n",
    "\n",
    "**MLR ONLY**:  \n",
    "- *Independence Part 2: The independent variables X_i and X_j must be independent of one another for any i != j*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SLR / MLR using sklearn package (Straight line)\n",
    "# PLR using sklearn package (Polynomial regression). (Curve line)\n",
    "# SVR Using sklearn (Support Vector Regression) = PLR curve line\n",
    "# Decision Tree Regression using sklearn. Not good one X. But better use for many X.\n",
    "# Random Forest Regression\n",
    "# using statsmodels, Do OLS model (ordinary least squares) = SLR\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_lasso = Lasso(random_state=0, alpha=500) # alpha 0 to 1\n",
    "lr_ridge = Ridge(alpha=100)   # alpha 0 to 1\n",
    "elast = ElasticNet(alpha=0.1, l1_ratio=0.1)   # alpha 0 to 1\n",
    "svr = SVR(kernel='linear')  # almost the same as poly\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0) # number of trees 10. Pick random.\n",
    "sgd_reg = SGDRegressor(random_state=0)\n",
    "gradboost_reg = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1) # default n_estimators=100, learning_rate=0.1\n",
    "\n",
    "print 'LinearRegression          : ', cross_val_score(lr, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'Lasso                     : ', cross_val_score(lr_lasso, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'Ridge                     : ', cross_val_score(lr_ridge, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'ElasticNet                : ', cross_val_score(elast, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'SVR                       : ', cross_val_score(svr, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'DecisionTreeRegressor     : ', cross_val_score(dtr, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'RandomForestRegressor     : ', cross_val_score(rfr, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print 'SDGRegressor              : ', cross_val_score(sgd_reg, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"GradientBoostingRegressor : \", cross_val_score(gradboost_reg, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "######################################################\n",
    "#Ensemble method ==> Bagging , AdaBoost, GradientBoost\n",
    "\n",
    "# Bagging regressor.\n",
    "lr = LinearRegression()\n",
    "lr_lasso = Lasso(random_state=0, alpha=500) # alpha 0 to 1\n",
    "lr_ridge = Ridge(alpha=100)   # alpha 0 to 1\n",
    "elast = ElasticNet(alpha=0.1, l1_ratio=0.1)   # alpha 0 to 1\n",
    "svr = SVR(kernel='linear')  # almost the same as poly\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0) # number of trees 10. Pick random.\n",
    "sgd_reg = SGDRegressor(random_state=0)\n",
    "gradboost_reg = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1) # default n_estimators=100, learning_rate=0.1\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "lr_bagger = BaggingRegressor(lr)\n",
    "lr_lasso_bagger = BaggingRegressor(lr_lasso)\n",
    "lr_ridge_bagger = BaggingRegressor(lr_ridge)\n",
    "elast_bagger = BaggingRegressor(elast)\n",
    "svr_bagger = BaggingRegressor(svr)\n",
    "dtr_bagger = BaggingRegressor(dtr)\n",
    "rfr_bagger = BaggingRegressor(rfr)\n",
    "sgd_reg_bagger = BaggingRegressor(sgd_reg)\n",
    "gradboost_reg_bagger = BaggingRegressor(gradboost_reg)\n",
    "\n",
    "print \"lr Bagging Score            : \", cross_val_score(lr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"lr_lasso_bagger  Score      : \", cross_val_score(lr_lasso_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"lr_ridge_bagger  Score      : \", cross_val_score(lr_ridge_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"elast_bagger  Score         : \", cross_val_score(elast_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"svr_bagger  Score           : \", cross_val_score(svr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"dtr_bagger  Score           : \", cross_val_score(dtr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"rfr_bagger  Score           : \", cross_val_score(rfr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"sgd_reg_bagger  Score       : \", cross_val_score(sgd_reg_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"gradboost_reg_bagger  Score : \", cross_val_score(gradboost_reg_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# AdaBoost Regressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "lr_adaboost = AdaBoostRegressor(base_estimator=lr, learning_rate=0.1 )\n",
    "lr_lasso_adaboost = AdaBoostRegressor(base_estimator=lr_lasso, learning_rate=0.1)\n",
    "lr_ridge_adaboost = AdaBoostRegressor(base_estimator=lr_ridge, learning_rate=0.1)\n",
    "elast_adaboost = AdaBoostRegressor(base_estimator=elast, learning_rate=0.1)\n",
    "svr_adaboost = AdaBoostRegressor(base_estimator=svr, learning_rate=0.1)\n",
    "dtr_adaboost = AdaBoostRegressor(base_estimator=dtr, learning_rate=0.1)\n",
    "rfr_adaboost = AdaBoostRegressor(base_estimator=rfr, learning_rate=0.1)\n",
    "sgd_reg_adaboost = AdaBoostRegressor(base_estimator=sgd_reg, learning_rate=0.1)\n",
    "gradboost_reg_adaboost = AdaBoostRegressor(base_estimator=gradboost_reg, learning_rate=0.1)\n",
    "\n",
    "\n",
    "print \"lr_adaboost Score       : \", cross_val_score(lr_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"lr_lasso_adaboost  Score: \", cross_val_score(lr_lasso_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"lr_ridge_adaboost  Score: \", cross_val_score(lr_ridge_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"elast_adaboost  Score   : \", cross_val_score(elast_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"svr_adaboost  Score     : \", cross_val_score(svr_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"dtr_adaboost  Score     : \", cross_val_score(dtr_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"rfr_adaboost  Score     : \", cross_val_score(rfr_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"sgd_reg_adaboost  Score : \", cross_val_score(sgd_reg_adaboost, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "#Regularization Youtube on Lasso L1, Ridge L2, Elastic Net (Lasso+Ridge)\n",
    "#https://www.youtube.com/watch?v=xyymDGReKdY\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(random_state=0, alpha=0.1) # alpha 0 to 1 #Method 2: Lasso regularization, L1\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1)   # alpha 0 to 1#Method 3: Ridge regularization\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasnet = ElasticNet(alpha=0.1, l1_ratio=0.1)   # alpha 0 to 1 #Method 4: Elastic regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (it predicts a number, that's why call regression)\n",
    "# Must do scaling before do this.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import GradientBoostingClassifier  \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "logr = LogisticRegression(random_state = 0)\n",
    "logr_lasso = LogisticRegressionCV(penalty='l1', solver='liblinear', Cs=100, cv=10)\n",
    "logr_ridge = LogisticRegressionCV(penalty='l2', Cs=200, cv=25) #l2 is ridge, Cs: How many different (automatically-selected) regularization strengths should be tested.\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2) # KNN = Neaest Neighbors classifier # choosig Euclidean('yoo-klid-ee-uhn') , the other type is called Manhattan\n",
    "dtc = DecisionTreeClassifier(max_depth=None)\n",
    "gbc = GradientBoostingClassifier(n_estimators=10) # warm_start=True meaning retain old training data when fit, but careful might overfit\n",
    "xgbc = XGBClassifier()  #Takes many model,\n",
    "\n",
    "logr.fit(Xvec_train, yc_train)\n",
    "logr_lasso.fit(Xvec_train, yc_train)\n",
    "logr_ridge.fit(Xvec_train, yc_train)\n",
    "knn.fit(Xvec_train, yc_train)\n",
    "dtc.fit(Xvec_train, yc_train)\n",
    "gbc.fit(Xvec_train, yc_train)\n",
    "xgbc.fit(Xvec_train, yc_train)\n",
    "\n",
    "print 'cross_val_score Scores'\n",
    "print 'LogisticRegression             : ' , cross_val_score(logr, X_test, y_test, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , cross_val_score(logr_lasso, y_test, yc, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , cross_val_score(logr_ridge, Xy_test_test, yc, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'KNeighborsClassifier           : ' , cross_val_score(knn, X_test, y_test, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'DecisionTreeClassifier         : ' , cross_val_score(dtc, X_test, y_test, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'GradientBoostClassifier        : ' , cross_val_score(gbc, X_test, y_test, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "print 'XGBClassifier                  : ' , cross_val_score(xgbc, X_test, y_test, cv=10, scoring='roc_auc', n_jobs=-1).mean()\n",
    "\n",
    "print 'Accuracy Scores 1: model.score(X_test,y_test)'\n",
    "print 'LogisticRegression             : ' , logr.score(X_test, y_test)\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , logr_lasso.score(X_test, y_test)\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , logr_ridge.score(X_test, y_test)\n",
    "print 'KNeighborsClassifier           : ' , knn.score(X_test, y_test)\n",
    "print 'DecisionTreeClassifier         : ' , dtc.score(X_test, y_test)\n",
    "print 'GradientBoostClassifier        : ' , gbc.score(X_test, y_test)\n",
    "print 'XGBClassifier                  : ' , xgbc.score(X_test, y_test)\n",
    "\n",
    "print 'Accuracy Scores 2: accuracy_score(X_test,y_test)'\n",
    "print 'LogisticRegression             : ' , accuracy_score(y_test, logr.predict(X_test))\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , accuracy_score(y_test, logr_lasso.predict(X_test))\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , accuracy_score(y_test, logr_ridge.predict(X_test))\n",
    "print 'KNeighborsClassifier           : ' , accuracy_score(y_test, knn.predict(X_test))\n",
    "print 'DecisionTreeClassifier         : ' , accuracy_score(y_test, dtc.predict(X_test))\n",
    "print 'GradientBoostClassifier        : ' , accuracy_score(y_test, gbc.predict(X_test))\n",
    "print 'XGBClassifier                  : ' , accuracy_score(y_test, xgbc.predict(X_test))\n",
    "\n",
    "print 'Confusion Matrix'\n",
    "print 'LogisticRegression              : ' , confusion_matrix(y_test, logr.predict(X_test))\n",
    "print 'LogisticRegressionCV L1 Lasso   : ' , confusion_matrix(y_test, logr_lasso.predict(X_test))\n",
    "print 'LogisticRegressionCV L2 Ridge   : ' , confusion_matrix(y_test, logr_ridge.predict(X_test))\n",
    "print 'KNeighborsClassifier            : ' , confusion_matrix(y_test, knn.predict(X_test))\n",
    "print 'DecisionTreeClassifier          : ' , confusion_matrix(y_test, dtc.predict(X_test))\n",
    "print 'GradientBoostClassifier         : ' , confusion_matrix(y_test, gbc.predict(X_test))\n",
    "print 'XGBClassifier                   : ' , confusion_matrix(y_test, xgbc.predict(X_test))\n",
    "\n",
    "print 'Classification Report (precision/recall/f1-score/support)\n",
    "print 'LogisticRegression             : ' , classification_report(y_test, logr.predict(X_test))\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , classification_report(y_test, logr_lasso.predict(X_test))\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , classification_report(y_test, logr_ridge.predict(X_test))\n",
    "print 'KNeighborsClassifier           : ' , classification_report(y_test, knn.predict(X_test))\n",
    "print 'DecisionTreeClassifier         : ' , classification_report(y_test, dtc.predict(X_test))\n",
    "print 'GradientBoostClassifier        : ' , classification_report(y_test, gbc.predict(X_test))\n",
    "print 'XGBClassifier                  : ' , classification_report(y_test, xgbc.predict(X_test))\n",
    "\n",
    "# ROC curve best view in plots\n",
    "# ROC = auc(fp,tp)\n",
    "\n",
    "fp_logr, tp_logr, _ = roc_curve(y_test, logr.predict(X_test)[:,1])\n",
    "fp_logr_lasso, tp_logr_lasso, _ = roc_curve(y_test, logr_lasso.predict(X_test)[:,1])\n",
    "fp_logr_ridge, tp_logr_ridge, _ = roc_curve(y_test, logr_ridge.predict(X_test)[:,1])\n",
    "fp_knn, tp_knn, _ = roc_curve(y_test, knn.predict(X_test)[:,1])\n",
    "fp_dtc, tp_dtc, _ = roc_curve(y_test, dtc.predict(X_test)[:,1])\n",
    "fp_gbc, tp_gbc, _ = roc_curve(y_test, gbc.predict(X_test)[:,1])\n",
    "fp_xgbc, tp_xgbc, _ = roc_curve(y_test, xgbc.predict(X_test)[:,1])\n",
    "\n",
    "print 'ROC curve. Best view in plots'\n",
    "print 'LogisticRegression             : ' , auc(fp_logr, tp_logr)\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , auc(fp_logr_lasso, tp_logr_lasso)\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , auc(fp_logr_ridge, tp_logr_ridge)\n",
    "print 'KNeighborsClassifier           : ' , auc(fp_knn, tp_knn)\n",
    "print 'DecisionTreeClassifier         : ' , auc(fp_dtc, tp_dtc)\n",
    "print 'GradientBoostClassifier        : ' , auc(fp_gbc, tp_gbc)\n",
    "print 'XGBClassifier                  : ' , auc(fp_xgbc, tp_xgbc)\n",
    "\n",
    "\n",
    "print 'Predicted Probabilties'\n",
    "print 'LogisticRegression             : ' , logr.predict_proba(Xc_test)\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , logr_lasso.predict_proba(Xc_test)\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , logr_ridge.predict_proba(Xc_test)\n",
    "print 'KNeighborsClassifier           : ' , knn.predict_proba(Xc_test)\n",
    "print 'DecisionTreeClassifier         : ' , dtc.predict_proba(Xc_test)\n",
    "print 'GradientBoostClassifier        : ' , gbc.predict_proba(Xc_test)\n",
    "print 'XGBClassifier                  : ' , xgbc.predict_proba(Xc_test)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "#Ensemble method ==> Bagging , AdaBoost, GradientBoost\n",
    "\n",
    "# Bagging Classifier.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "logrc_bagger = BaggingClassifier(logr)\n",
    "logrc_lasso_bagger = BaggingClassifier(logr_lasso)\n",
    "logrc_ridge_bagger = BaggingClassifier(logr_ridge)\n",
    "knn_bagger = BaggingClassifier(knn)\n",
    "dtc_bagger = BaggingClassifier(dtc)\n",
    "gbc_bagger = BaggingClassifier(gbc)\n",
    "xgbc_bagger = BaggingClassifier(xgbc)\n",
    "\n",
    "print \"logrc_bagger Score            : \", cross_val_score(logrc_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"logrc_lasso_bagger  Score      : \", cross_val_score(logrc_lasso_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"logrc_ridge_bagger  Score      : \", cross_val_score(logrc_ridge_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"knn_bagger  Score         : \", cross_val_score(elast_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"dtc_bagger  Score           : \", cross_val_score(svr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"gbc_bagger  Score           : \", cross_val_score(dtr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"rfr_bagger  Score           : \", cross_val_score(rfr_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "print \"xgbc_bagger  Score       : \", cross_val_score(sgd_reg_bagger, Xs, y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---Not important info---------------------------------------------\n",
    "print 'Lamda value (penalty)   '\n",
    "print 'LogisticRegression             : ' , logr.C_\n",
    "print 'LogisticRegressionCV L1 Lasso  : ' , logr_lasso.C_\n",
    "print 'LogisticRegressionCV L2 Ridge  : ' , logr_ridge.C_\n",
    "print 'KNeighborsClassifier           : ' , knn.C_\n",
    "print 'DecisionTreeClassifier         : ' , dtc.C_\n",
    "print 'GradientBoostClassifier        : ' , gbc.C_\n",
    "print 'XGBClassifier                  : ' , xgbc.C_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering (choosing K number of clusters is important.)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "k_means = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "# After we fit our data, we can get our predicted labels from model.labels_ and the center pointsmodel.cluster_centers_.\n",
    "predicted = model.labels_   # Predicted clusters to points\n",
    "centroids = model.cluster_centers_   #Location of centroids\n",
    "score = silhouette_score(df, predicted, metric='euclidean')  # measure of how far apart clusters are. ranges from (too close)-1 to 1 (well separated)\n",
    "\n",
    "#####################################################################################\n",
    "# Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "#####################################################################################\n",
    "DBSCAN\n",
    "http://localhost:8888/notebooks/Downloads/classes/week-08/labs/clustering-dbscan-lab-master/solution-code/practice-dbscan-solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "bagger = BaggingClassifier(clf)\n",
    "print \"DT Score:\\t\", cross_val_score(clf, X, y, cv=10, n_jobs=1).mean()\n",
    "print \"Bagging Score:\\t\", cross_val_score(bagger, X, y, cv=10, n_jobs=1, n_estimators =5, max_features = 0.5, max_samples = 0.5).mean()\n",
    "\n",
    "# AdaBoost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(base_estimator=None, n_estimators=100) # default is random forest\n",
    "ada.fit(X,y)\n",
    "ada.score\n",
    "\n",
    "\n",
    "# Voting Classifier (takes all models and outputs the best)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "vote = VotingClassifier(\n",
    "                          estimators = [('lr', LogisticRegression()),\n",
    "                                       ('rf', RandomForestClassifier()),\n",
    "                                        ('gnb', GaussianNB()) \n",
    "                                       ], \n",
    "                          voting = 'hard')\n",
    "\n",
    "\n",
    "\n",
    "                                       \n",
    "# Bagging on KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "bagger_clf = BaggingClassifier(knn_clf)\n",
    "print \"Knn Score:\\t\", cross_val_score(knn_clf, knn_X, knn_y, cv=10, n_jobs=1).mean()\n",
    "print \"Bagging Score:\\t\", cross_val_score(bagger_clf, knn_X, knn_y, cv=10, n_jobs=1, n_estimators =5, max_features = 0.5, max_samples = 0.5).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1: Backward Step feature reduction or feature selection via statemodel\n",
    "# ordinary least squares OLS model, aka SLR, using statsmodels api module\n",
    "import statsmodels.formula.api as smf\n",
    "X_opt = X[ : , [0,1,2,3,4,5]]\n",
    "model_OLS = smf.OLS(endog = y, exog = X_opt).fit()\n",
    "model_OLS.summary()\n",
    "# Read each P_value. If P_value > 0.5, eliminate the column.\n",
    "# Repeat above until all columns's P_value is lower than 0.5.\n",
    "# Watch R2 and Adjusted R2 during dropping variables, trick is, stop dropping when adjusted R2 started to decreases.\n",
    "############################################################################\n",
    "# logistic regression\n",
    "import statsmodels.formula.api as sm\n",
    "news_data = data[['label','is_news']]\n",
    "news_model = sm.logit(\"label ~ is_news\", data=news_data).fit()\n",
    "#more_cat_model = sm.logit(\"label ~ C(alchemy_category, Treatment(reference='unknown'))\", data=data).fit()\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "news_model.summary()\n",
    "# To interpret summary, see coeff(-ve/+ve) and p_value(good if less than 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Searching"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results.param_grid\t       Displays parameters searched over.\n",
    "results.best_score_\t       Best mean cross-validated score achieved.\n",
    "results.best_estimator_\t   Reference to model with best score. Is usable / callable.\n",
    "results.best_params_\t   The parameters that have been found to perform with the best score.\n",
    "results.grid_scores_\t    Display score attributes with corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random GridSearch on RandomForestRegressor\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "Step 1: Do Random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] # Number of trees in random forest\n",
    "max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]  # Minimum number of samples required to split a node\n",
    "min_samples_leaf = [1, 2, 4]  # Minimum number of samples required at each leaf node\n",
    "bootstrap = [True, False]  # Method of selecting samples for training each tree\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "rf = RandomForestRegressor()    # First create the base model to tune\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X[selected_best_cols], y) # Pls use best selected cols\n",
    "# Shift Enter, when finished do this:\n",
    "rf_random.best_params_\n",
    "#--------------------------------\n",
    "#Step 2: Do baseline Grid Search, using the best params above\n",
    "#---------------------------------    \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [100, 500, 1000, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [0.01, 0.25, 0.5, 0.75],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'n_estimators': [400, 450, 500, 550, 570]\n",
    "}\n",
    "# Create a based model\n",
    "rf_based = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_based, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X[lassoCV_columns], y)\n",
    "print 'rf_based.best_params_ : ', rf_based.best_params_\n",
    "print 'rf_based.best_score_ : ', rf_based.best_score_\n",
    "\n",
    "#--------------------------------\n",
    "# GridSearch on GradientBoostingRegressor\n",
    "# Step 1: Do a random search \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Create the random grid\n",
    "random_gboost={'n_estimators':[100,500,1000], \n",
    "            'learning_rate': [0.1,0.05,0.02, 0.01], \n",
    "            'max_depth':[4,6], \n",
    "            'min_samples_leaf':[3,5,9,17], \n",
    "            'max_features':[1.0,0.3,0.1] } \n",
    "\n",
    "gradboost = GradientBoostingRegressor()\n",
    "gb_random = RandomizedSearchCV(estimator = gradboost, param_distributions = random_gboost, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "gb_random.fit(X[lassoCV_columns], y)\n",
    "print 'xgb_random.best_params_ : ', gb_random.best_params_\n",
    "print 'xgb_random.best_score_ : ', gb_random.best_score_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search in KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "knn_params = {\n",
    "    'n_neighbors':[1,3,5,9,15,21],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1,n_jobs=-1)\n",
    "knn_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Examine Result KNN\n",
    "print 'knn_gridsearch.best_score_' , knn_gridsearch.best_score_\n",
    "print 'knn_gridsearch.best_params_', knn_gridsearch.best_params_\n",
    "best_knn = knn_gridsearch.best_estimator_\n",
    "print 'best_knn' , best_knn\n",
    "print 'best_knn' , best_knn.score(X_test, y_test)\n",
    "print 'baseline:', np.mean(y_test)\n",
    "#print 'default KNN:', knn.score(X_test, y_test) # to check against normal KNN\n",
    "\n",
    "# Grid Search in LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "gs_params = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'solver':['liblinear'],\n",
    "    'C':np.logspace(-5,0,100)\n",
    "}\n",
    "\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), gs_params, cv=5, verbose=1,n_jobs=-1)\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Examine Result LogisticRegression\n",
    "lr_gridsearch.best_score_\n",
    "lr_gridsearch.best_params_\n",
    "best_lr = lr_gridsearch.best_estimator_\n",
    "best_lr.score(X_test, y_test)\n",
    "coef_df = pd.DataFrame({\n",
    "        'coef':best_lr.coef_[0],\n",
    "        'feature':X.columns\n",
    "    })\n",
    "coef_df['abs_coef'] = np.abs(coef_df.coef)\n",
    "# sort by absolute value of coefficient (magnitude)\n",
    "coef_df.sort_values('abs_coef', ascending=False, inplace=True)\n",
    "# Show non-zero coefs and predictors\n",
    "coef_df[coef_df.coef != 0]\n",
    "\n",
    "\n",
    "# Gridsearch on DecisionTreeClassifier\n",
    "# gridsearch params\n",
    "dtc_params = {\n",
    "    'max_depth':[None,1,2,3,4],\n",
    "    'max_features':[None,'log2','sqrt',2,3,4,5],\n",
    "    'min_samples_split':[2,3,4,5,10,15,20,25,30,40,50]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# set the gridsearch\n",
    "dtc_gs = GridSearchCV(DecisionTreeClassifier(), \n",
    "                      dtc_params, \n",
    "                      cv=5, \n",
    "                      verbose=1, \n",
    "                      scoring='roc_auc', \n",
    "                      n_jobs=-1)\n",
    "\n",
    "# use the gridearc C model to fit the data\n",
    "dtc_gs.fit(X, y)\n",
    "dtc_best = dtc_gs.best_estimator_\n",
    "print dtc_gs.best_params_\n",
    "print dtc_gs.best_score_\n",
    "# Print out \"feature importances\"\n",
    "fi = pd.DataFrame({\n",
    "        'feature':X.columns,\n",
    "        'importance':dtc_best.feature_importances_\n",
    "    })\n",
    "\n",
    "fi.sort_values('importance', ascending=True, inplace=True)\n",
    "#fi\n",
    "\n",
    "fi.plot(kind='barh')\n",
    "plt.yticks(range((len(fi))), fi['feature'])\n",
    "plt.show()\n",
    "\n",
    "#########################################\n",
    "# ot run multiple experiment via Gridsearch.\n",
    "dtc_gs = GridSearchCV(DecisionTreeClassifier(), \n",
    "                      dtc_params, \n",
    "                      cv=5, \n",
    "                      verbose=0, \n",
    "                      scoring='roc_auc', \n",
    "                      n_jobs=-1)\n",
    "\n",
    "for _ in range(5):\n",
    "    dtc_gs.fit(X, y)\n",
    "    dtc_best = dtc_gs.best_estimator_\n",
    "    print(dtc_gs.best_params_)\n",
    "    print(dtc_gs.best_score_)\n",
    "    fi = pd.DataFrame({\n",
    "        'feature':X.columns,\n",
    "        'importance':dtc_best.feature_importances_\n",
    "    })\n",
    "\n",
    "    fi.sort_values('importance', ascending=True, inplace=True)\n",
    "    fi.plot(kind='barh')\n",
    "    plt.yticks(range((len(fi))), fi['feature'])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "######################################################\n",
    "Gridsearch SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = svm.SVC()\n",
    "\n",
    "gamma_range = np.logspace(-5, 2, 10)\n",
    "C_range = np.logspace(-3, 2, 10)\n",
    "kernel_range = ['rbf', 'sigmoid', 'linear', 'poly']\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range, kernel=kernel_range)\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid, cv=3, scoring='accuracy', verbose=1,n_jobs=-1) # scoring= roc_auc\n",
    "grid.fit(digits_X, digits_y)\n",
    "\n",
    "# check the results of the grid search\n",
    "print grid.best_params_\n",
    "print grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline (must be a function with transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "quote_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('logit', LogisticRegression())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation or Metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Classification: Accuracy, Precision, Recall, F1, ROC/AUC, Confusion Matrix.\n",
    "1) Accuracy = No. of Predicted Correctly / Total no. of samples\n",
    "But Accuracy is not good for FN and FP, so we need Precision and Recall \n",
    "2) Precision = TP/(TP+FP) = 1 is good model\n",
    "3) Recall = TP/(TP+FN) = 1 is good model\n",
    "But if system has Precision><Recall, system is not good. So we have F-measure\n",
    "4) F1 = formula = 1 is good model measure taking Precision and Recall into account.\n",
    "\n",
    "#### Regression : Adjusted R2, R2, RMSE/MSE , MAE Mean Absolute Error, SSE Sum of squares error, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_lasso = Lasso(random_state=0, alpha=500) # alpha 0 to 1\n",
    "lr_ridge = Ridge(alpha=100)   # alpha 0 to 1\n",
    "elast = ElasticNet(alpha=0.1, l1_ratio=0.1)   # alpha 0 to 1\n",
    "svr = SVR(kernel='linear')  # almost the same as poly\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0) # number of trees 10. Pick random.\n",
    "gradboost_reg = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1) # default n_estimators=100, learning_rate=0.1\n",
    "\n",
    "lr.fit(Xs_train, y_train)\n",
    "lr_lasso.fit(Xs_train, y_train)\n",
    "lr_ridge.fit(Xs_train, y_train)\n",
    "elast.fit(Xs_train, y_train)\n",
    "svr.fit(Xs_train, y_train)\n",
    "dtr.fit(Xs_train, y_train)\n",
    "rfr.fit(Xs_train, y_train)\n",
    "gradboost_reg.fit(Xs_train, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(Xs_test)\n",
    "y_pred_lr_lasso = lr_lasso.predict(Xs_test)\n",
    "y_pred_lr_ridge = lr_ridge.predict(Xs_test)\n",
    "y_pred_elast = elast.predict(Xs_test)\n",
    "y_pred_svr = svr.predict(Xs_test)\n",
    "y_pred_dtr = dtr.predict(Xs_test)\n",
    "y_pred_rfr = rfr.predict(Xs_test)\n",
    "y_pred_gradboost_reg = gradboost_reg.predict(Xs_test)\n",
    "\n",
    "print '-----------Cross Val Score---------------'\n",
    "print 'LinearRegression          : ', cross_val_score(lr, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'Lasso                     : ', cross_val_score(lr_lasso, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'Ridge                     : ', cross_val_score(lr_ridge, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'ElasticNet                : ', cross_val_score(elast, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'SVR                       : ', cross_val_score(svr, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'DecisionTreeRegressor     : ', cross_val_score(dtr, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print 'RandomForestRegressor     : ', cross_val_score(rfr, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "print \"GradientBoostingRegressor : \", cross_val_score(gradboost_reg, Xs_train, y_train, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "print '-----------Mean Absolute Error---------------'\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print 'LinearRegression          : ', mean_absolute_error(y_test, y_pred_lr)\n",
    "print 'Lasso                     : ', mean_absolute_error(y_test, y_pred_lr_lasso)\n",
    "print 'Ridge                     : ', mean_absolute_error(y_test, y_pred_lr_ridge)\n",
    "print 'ElasticNet                : ', mean_absolute_error(y_test, y_pred_elast)\n",
    "print 'SVR                       : ', mean_absolute_error(y_test, y_pred_svr)\n",
    "print 'DecisionTreeRegressor     : ', mean_absolute_error(y_test, y_pred_dtr)\n",
    "print 'RandomForestRegressor     : ', mean_absolute_error(y_test, y_pred_rfr)\n",
    "print \"GradientBoostingRegressor : \", mean_absolute_error(y_test, y_pred_gradboost_reg)\n",
    "\n",
    "print '-----------RMSE Root Mean Square Error---------------'\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print 'LinearRegression          : ', np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print 'Lasso                     : ', np.sqrt(mean_squared_error(y_test, y_pred_lr_lasso))\n",
    "print 'Ridge                     : ', np.sqrt(mean_squared_error(y_test, y_pred_lr_ridge))\n",
    "print 'ElasticNet                : ', np.sqrt(mean_squared_error(y_test, y_pred_elast))\n",
    "print 'SVR                       : ', np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "print 'DecisionTreeRegressor     : ', np.sqrt(mean_squared_error(y_test, y_pred_dtr))\n",
    "print 'RandomForestRegressor     : ', np.sqrt(mean_squared_error(y_test, y_pred_rfr))\n",
    "print \"GradientBoostingRegressor : \", np.sqrt(mean_squared_error(y_test, y_pred_gradboost_reg))\n",
    "\n",
    "print '-----------R Score---------------'\n",
    "from sklearn.metrics import r2_score\n",
    "print 'LinearRegression          : ', r2_score(y_test, y_pred_lr)\n",
    "print 'Lasso                     : ', r2_score(y_test, y_pred_lr_lasso)\n",
    "print 'Ridge                     : ', r2_score(y_test, y_pred_lr_ridge)\n",
    "print 'ElasticNet                : ', r2_score(y_test, y_pred_elast)\n",
    "print 'SVR                       : ', r2_score(y_test, y_pred_svr)\n",
    "print 'DecisionTreeRegressor     : ', r2_score(y_test, y_pred_dtr)\n",
    "print 'RandomForestRegressor     : ', r2_score(y_test, y_pred_rfr)\n",
    "print \"GradientBoostingRegressor : \", r2_score(y_test, y_pred_gradboost_reg)\n",
    "\n",
    "####################################################################################################################\n",
    "\n",
    "# For Classification\n",
    "\n",
    "# Accuracy Score 1\n",
    "knn.score(X_test, y_test)\n",
    "\n",
    "# Accuracy Score 2\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    " \n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ROC curve best view in plots\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, yhat_pp[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "# For Clustering\n",
    "\n",
    "Adjusted Rand Index\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "Homogeneity\n",
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity_score(y_true, y_pred)\n",
    "\n",
    "V-measure\n",
    "from sklearn.metrics import v_measure_score\n",
    "metrics.v_measure_score(y_true, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "https://pandas.pydata.org/pandas-docs/stable/visualization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Plot\n",
    "# kind='scatter', color='dodgerblue', figsize=(15,7), s=250,  fontsize=14\n",
    "# style={'col1': ':r', 'col4': 'vb'}, style={'ZN': 'g', 'INDUS':'--b'}\n",
    "# stacked = True\n",
    "\n",
    "df['col1'].plot()\n",
    "df.plot(x='col1', y='col2)\n",
    "df[['col1', 'col4']].plot()\n",
    "df.iloc[5].plot.barh()\n",
    "df.hist(bins=20)   # to plot historgram\n",
    "        \n",
    "# Mathlab SUbplot (Many plots side by side)\n",
    "fig, axes = plt.subplots(2,3)\n",
    "df.plot(ax=axes[0][0])\n",
    "df['col1'].plot(ax=axes[0][1])\n",
    "df['col2'].plot(ax=axes[1][1])\n",
    "        \n",
    "\n",
    "# Noel ax=df.plot must be on TOP\n",
    "# Noel [['if 2x' ,  '2x columns']]  ==> 2x blackets\n",
    "ax = df[['col1','col2']].plot(kind='bar', figsize=(15,3)) \n",
    "ax.set_title('Some Kinda Plot Thingy', fontsize=21, y=1.01)\n",
    "ax.legend(loc=1)                         #1,2,3,4 corners location)\n",
    "ax.set_ylabel('Important y-axis info', fontsize=16)\n",
    "ax.set_xlabel('Meaningless x-axis info', fontsize=16)\n",
    "\n",
    "# # Seaborn \n",
    "sns.pairplot(df)\n",
    "sns.heatmap(df)   # df.corr()**2, annot=True, center=0\n",
    "sns.jointplot(df)\n",
    "sns.lmplot(df)       # hue='chain\n",
    "sns.distplot(df)\n",
    "sns.violinplot(df)\n",
    "sns.regplot(x, y)\n",
    "sns.factorplot(x, y, df, kind='bar') # x = 'col'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE CODE for DecisionTreeRegressor\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# initialize the output file object\n",
    "# max_depth = none \n",
    "# keeps creating trees until purity = 1\n",
    "dot_data = StringIO()  \n",
    "\n",
    "export_graphviz(dtrN, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names=Xr.columns)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
